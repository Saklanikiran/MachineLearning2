{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2727050-40eb-477e-8702-f180385fa7ac",
   "metadata": {},
   "source": [
    "# Ans : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab1b9a4-d848-4e26-875e-eefdb2ec18bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n Underfitting occurs when a model is too simple to capture the underlying patterns in the training data. It fails to learn \\n the training data and performs poorly on both the training set and new data.\\n \\n>> Consequences :\\n  Inability to capture patterns: The model lacks the complexity to represent the underlying relationships in the data.\\n  Poor performance on training and test data: The model doesn't fit the data well, resulting in low accuracy.\\n  \\n>> Increase model complexity: Use a more complex model with more parameters to capture intricate patterns in the data.\\n  Feature engineering: Create new features or transform existing ones to provide more information to the model.\\n  Add more data: Increase the size of the training dataset to give the model more examples to learn from.\\n  \\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overfitting\n",
    "\n",
    "'''\n",
    "Overfitting occurs when a model learns the training data too well, capturing noise and random fluctuations in the data rather \n",
    "than the underlying patterns. As a result, the model performs well on the training data but poorly on new, unseen data.\n",
    "\n",
    ">> Consequences:\n",
    " Poor generalization: The model may not perform well on real-world data because it has essentially memorized the training set.\n",
    " Sensitivity to noise: Small variations in the training data can lead to significant changes in the model, \n",
    "                      making it less robust.\n",
    "            \n",
    ">> Mitigation :\n",
    "  Regularization: Introduce penalties for complex models by adding regularization terms to the loss function.\n",
    "  Cross-validation: Split the dataset into multiple subsets for training and validation to get a better estimate of the model's performance on unseen data.\n",
    "  Feature selection: Remove irrelevant or redundant features that may contribute to overfitting.\n",
    "\n",
    "'''\n",
    "\n",
    "# Underfitting:\n",
    "\n",
    "'''\n",
    " Underfitting occurs when a model is too simple to capture the underlying patterns in the training data. It fails to learn \n",
    " the training data and performs poorly on both the training set and new data.\n",
    " \n",
    ">> Consequences :\n",
    "  Inability to capture patterns: The model lacks the complexity to represent the underlying relationships in the data.\n",
    "  Poor performance on training and test data: The model doesn't fit the data well, resulting in low accuracy.\n",
    "  \n",
    ">> Increase model complexity: Use a more complex model with more parameters to capture intricate patterns in the data.\n",
    "  Feature engineering: Create new features or transform existing ones to provide more information to the model.\n",
    "  Add more data: Increase the size of the training dataset to give the model more examples to learn from.\n",
    "  \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf7a5e-8fae-428e-a2c2-cc45d317f397",
   "metadata": {},
   "source": [
    "# Ans : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0edbad26-2302-483e-b24b-8c3fd9b4218e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReducing overfitting in machine learning involves employing various techniques to prevent a model from \\nfitting the training data too closely and improving its generalization to unseen data.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Reducing overfitting in machine learning involves employing various techniques to prevent a model from \n",
    "fitting the training data too closely and improving its generalization to unseen data.\n",
    "'''\n",
    "# Strategies for reducing overfitting :\n",
    "# 1.Regularization: Regularization techniques, such as L1 and L2 regularization, add penalty terms to the model's loss function based on the magnitude of its parameters.\n",
    "# 2.Cross-Validation :Use techniques like k-fold cross-validation to evaluate the model's performance on multiple subsets of the data.\n",
    "# 3.Feature Selection: Remove irrelevant or redundant features from the dataset.\n",
    "# 4.Ensemble Learning: Data augmentation helps expose the model to a wider variety of patterns and reduces the likelihood of overfitting.\n",
    "# 5.Ensemble Learning: Train multiple models and combine their predictions to improve overall performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e117ea-3163-4dba-8d73-9cf3309b3144",
   "metadata": {},
   "source": [
    "# Ans : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95dbef57-88e2-480f-bf7b-0075aca14d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n> Simple Models with Insufficient Capacity\\n> Insufficient Training Time\\n> Inadequate Feature Representation\\n> Small Training Dataset\\n> Ignoring Important Features\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Underfitting : Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the\n",
    "               training data. The model fails to learn the relationships and structures present in the data, resulting in \n",
    "               poor performance on both the training set and new, unseen data. Underfitting can occur in various scenarios,\n",
    "               often when the model lacks the necessary complexity to represent the underlying data distribution. \n",
    "\n",
    "'''\n",
    "# scenarios where underfitting can occur in ML :\n",
    "'''\n",
    "> Simple Models with Insufficient Capacity\n",
    "> Insufficient Training Time\n",
    "> Inadequate Feature Representation\n",
    "> Small Training Dataset\n",
    "> Ignoring Important Features\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4846968a-4bee-4480-a3ec-13c920b1009b",
   "metadata": {},
   "source": [
    "# Ans : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f330a073-b775-4a0c-b9a7-4994464a0c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n>  High bias can result in underfitting, where the model fails to capture the true complexity of the data. \\n   This leads to poor performance on both the training data and new, unseen data.\\n>  High variance can result in overfitting, where the model fits the training data too closely but fails to generalize\\n   to new data. This leads to excellent performance on the training set but poor performance on unseen data.\\n   \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bias-variance tradeoff :\n",
    "'''\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that refers to the balance between bias and variance \n",
    "in a model. Both bias and variance are sources of error that contribute to a model's performance, and finding the right \n",
    "tradeoff is crucial for building models that generalize well to new, unseen data.\n",
    "\n",
    "> The bias-variance tradeoff arises because, in general, increasing model complexity reduces bias but increases variance, \n",
    "  and vice versa.\n",
    "> A simple model with few parameters is more likely to have high bias and low variance. It may not capture the underlying\n",
    "  patterns well, resulting in underfitting.\n",
    "> A complex model with many parameters is more likely to have low bias but high variance. It may fit the training data very\n",
    "  closely, including noise, resulting in overfitting.\n",
    "\n",
    "'''\n",
    "\n",
    "# Relationship between bias and variance :\n",
    "\n",
    "'''\n",
    "> There is an inverse relationship between bias and variance. As one decreases, the other tends to increase.\n",
    "> Achieving a good tradeoff involves finding the right level of model complexity that minimizes the total error\n",
    "  on new, unseen data, which is the sum of bias and variance.\n",
    "'''\n",
    "# Effect model prediction :\n",
    "'''\n",
    ">  High bias can result in underfitting, where the model fails to capture the true complexity of the data. \n",
    "   This leads to poor performance on both the training data and new, unseen data.\n",
    ">  High variance can result in overfitting, where the model fits the training data too closely but fails to generalize\n",
    "   to new data. This leads to excellent performance on the training set but poor performance on unseen data.\n",
    "   \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fa0610-fe05-4903-bd6c-53e4861f8105",
   "metadata": {},
   "source": [
    "# Ans : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6fc9d7-6fc1-4c6d-84da-c55bfab1ac27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n > By using a combination of methods, we can gain insights into whether the model is overfitting or underfitting \\n   and take appropriate steps to improve its performance.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method for overfitting and underfitting in machine learning :\n",
    "'''\n",
    "1.Learning Curves:\n",
    "    > Overfitting: On a learning curve, an overfit model will have a large gap between training and validation performance.\n",
    "      The training performance may be very high, while the validation performance plateaus or starts to degrade.\n",
    "    > Underfitting: In contrast, an underfit model will have low performance on both the training and validation sets,\n",
    "      and the learning curves may not converge.\n",
    "      \n",
    "2.Model Evaluation Metrics:\n",
    "    > Overfitting: If a model performs exceptionally well on the training set but poorly on the validation set,\n",
    "      it may be overfitting. Common metrics include accuracy, precision, recall, or F1 score.\n",
    "    > Underfitting: Low performance on both the training and validation sets may indicate underfitting.\n",
    "    \n",
    "3.Validation and Test Sets:\n",
    "    > Overfitting: If the model performs significantly better on the training set than on a separate validation or test set,\n",
    "      it suggests overfitting.\n",
    "    > Underfitting: Consistently low performance on both the training and validation/test sets may indicate underfitting.\n",
    "    \n",
    "4.Cross-Validation:\n",
    "    > Overfitting: Evaluate the model using cross-validation to get a more reliable estimate of its performance on different\n",
    "      subsets of the data. Overfit models may show high variability in performance across folds.\n",
    "    > Underfitting: Cross-validation can reveal consistently poor performance across folds.\n",
    "    \n",
    "'''\n",
    "'''\n",
    " > By using a combination of methods, we can gain insights into whether the model is overfitting or underfitting \n",
    "   and take appropriate steps to improve its performance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b0663-aea7-4a29-b210-a714f79cac8f",
   "metadata": {},
   "source": [
    "# Ans : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0ae740-c410-49c2-8deb-ab91b8e73d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n>> Differ in terms of performance :\\n\\n  ->> High Bias (Underfitting):\\n        Training Set: Poor performance due to oversimplification.\\n        Test Set: Similar poor performance due to the model's inability to capture underlying patterns.\\n    \\n  ->> High Variance (Overfitting):\\n        Training Set: Excellent performance as the model fits the data closely.\\n        Test Set: Poor performance due to the model's inability to generalize to new, unseen data.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare and contrase bias and variance :\n",
    "'''\n",
    ">> Bias :\n",
    "    -> Bias refers to the error introduced by approximating a real-world problem too simplistically.\n",
    "       It represents the difference between the model's predictions and the true values.\n",
    "    -> High bias leads to underfitting, where the model cannot capture the underlying patterns in the data,\n",
    "       resulting in poor performance on both the training set and new, unseen data.\n",
    ">> variance : \n",
    "    -> Variance represents the model's sensitivity to small fluctuations or noise in the training data. \n",
    "       It measures how much the model's predictions would change if trained on a different dataset.\n",
    "    -> High variance leads to overfitting, where the model fits the training data too closely, capturing noise and\n",
    "       leading to excellent performance on the training set but poor performance on unseen data.\n",
    "       \n",
    "'''\n",
    "# Example of high bias : A linear regression model applied to a non-linear relationship in the data.\n",
    "# Example of high variance : A high-degree polynomial regression model.\n",
    "\n",
    "'''\n",
    ">> Differ in terms of performance :\n",
    "\n",
    "  ->> High Bias (Underfitting):\n",
    "        Training Set: Poor performance due to oversimplification.\n",
    "        Test Set: Similar poor performance due to the model's inability to capture underlying patterns.\n",
    "    \n",
    "  ->> High Variance (Overfitting):\n",
    "        Training Set: Excellent performance as the model fits the data closely.\n",
    "        Test Set: Poor performance due to the model's inability to generalize to new, unseen data.\n",
    "'''\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0d37d-fe7d-4135-9646-64316cb0e500",
   "metadata": {},
   "source": [
    "# Ans : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a446f648-3e30-4b9f-b149-2a9ac7384159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization in Machine Learning:\n",
    "'''\n",
    "Regularization is a technique in machine learning used to prevent overfitting and improve the generalization of a model\n",
    "to new, unseen data. Overfitting occurs when a model fits the training data too closely, capturing noise and producing \n",
    "poor performance on new data. Regularization introduces a penalty term to the model's loss function, discouraging overly\n",
    "complex models with large parameter values.\n",
    "'''\n",
    "\n",
    "# Regularization Techniques:\n",
    "\n",
    "'''\n",
    "Dropout (Neural Networks):\n",
    "Implementation: Randomly drop (set to zero) a fraction of neurons during each training iteration.\n",
    "Effect: Prevents neurons from relying too heavily on specific input features, enhancing generalization.\n",
    "Use Case: Commonly used in neural networks.\n",
    "\n",
    "\n",
    "Early Stopping:\n",
    "Implementation: Monitor the model's performance on a validation set during training and stop training once performance on the validation set starts to degrade.\n",
    "Effect: Prevents the model from continuing to learn noise in the training data.\n",
    "Use Case: Particularly useful when training deep neural networks.\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
